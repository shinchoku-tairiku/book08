@import: ../../shinchoku-tairiku.satyh/shinchoku-tairiku

let-block +nomaddo = '<
  +chapter{ONNX: Deep Learning 中間フォーマットという荒野}{もう疲れたよパトラッシュ}<

  +p {
      皆さんは機械学習してますか？ してますよね？
      特に今イケているナウでヤングな皆さんは深層学習モデルを取り扱っているかと思います。
      その中でONNX(オニキスと読む？)と呼ばれる深層学習モデルの計算グラフを扱うフォーマットについてつらつらと書いていきたいと思います。
      あまり結論があるような話ではないですが楽しんでもらえればなによりです。
    }

    +section?:(`nomaddo:what_is_ONNX`){ONNXとは？なぜ必要？}<
      +p {
       Open Neural Network Exchange (以後ONNXと書きます) とは主にMicrosoftが主導して
       \footnote{実際はパートナー企業が他にもいるけど積極的なのはMicrosoftです。後述のONNXRUNTIMEやONNX.jsなどを開発しています。彼らのクラウドサービスで扱うための中間フォーマットとして使うためですね。} 開発している深層学習モデルの中間フォーマットです。
      }

      +p {
       深層学習モデルは計算内容が下の図のように計算の連鎖が副作用のないグラフとして表現されます。\footnote{LSTMとか副作用のあるグラフもありますが……大体は副作用が無いです。}
      }

      +figure?:(`nomaddo:fig1`)?:(3cm)(`./no_maddo/computation_graph.pdf`){計算をグラフとして表す}

      +p {
        Pytorch・KerasなどPythonのDLフレームワークを使う・Neural Network Consoleなどのビジュアルエディタを用いる・AutoMLなどの自動モデル構築サービスを使うなど
        深層学習モデルを記述する方法は色々ありますが、本質的な部分では
        計算グラフというものは記述に用いるフレームワークに依存しないはずです。
      }

      +p {
        無論学習方法などはフレームワークごとに異なるのは普通だと思いますが、
        学習が終わったモデルとそのパラメータは学習に用いたフレームワークに非依存
        にしたいはずです。Pythonで書かれたプログラムを学習に用いても、
        動かしたい環境は多岐に渡ります。
      }

      +p {
        例えば今想定される動作環境は以下のようなものがあります。
      }

      +enumerate {
        * NVIDIA GPUを搭載したサーバ環境
        * Intel HD Graphicsを搭載したノートブックのネイティブアプリケーション
        * Mali GPU / Adreno GPU などモバイルGPU搭載のAndroidデバイス
        * ブラウザ環境
        * 1チップ100円以下のマイコン（？）
        * FPGA上での実装（？）
      }

      +p {
        そこで計算内容を計算環境に非依存のフォーマットとランタイムを分離し、
        実行環境に合わせてランタイムが高速化するようなモデルが求められるわけです。
      }

      +p {
        ONNXの実行環境として有力なものに
      }
    >
  >
>