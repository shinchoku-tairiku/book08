@import: ../../shinchoku-tairiku.satyh/shinchoku-tairiku

let-block +nomaddo = '<
  +chapter{ONNXという荒野}{もう疲れたよパトラッシュ}<
    +p {
        こんばんは、インターネットの闇@no\_maddo です。普段は深層学習関連のソフトウエア開発やらコンパイラやらを作っています。
    }
    +p {
        皆さんは機械学習してますか？ してますよね？
        特に今イケているナウでヤングな皆さんは深層学習モデルを取り扱っているかと思います。
        その中でONNX(オニキスと読む？)と呼ばれる深層学習モデルの計算グラフを扱うフォーマットについてつらつらと書いていきたいと思います。
        あまり結論があるような話ではないですが楽しんでもらえればなによりです。
      }

    +section?:(`nomaddo:what_is_ONNX`){ONNXとは？なぜ必要？}<
      +p {
       Open Neural Network Exchange \footnote{\url(`https://onnx.ai`);} (以後ONNXと書きます) とは主にMicrosoftが主導して
       \footnote{実際はパートナー企業が他にもいるけど積極的なのはMicrosoftです。後述のONNXRUNTIMEやONNX.jsなどを開発しています。彼らのクラウドサービスで扱うための中間フォーマットとして使うためですね。} 開発している深層学習モデルの中間フォーマットです。
      }

      +p {
       深層学習モデルは計算内容が下の図のように計算の連鎖が副作用のないグラフとして表現されます。\footnote{LSTMとか副作用のあるグラフもありますが……大体は副作用が無いです。}
      }

      +figure?:(`nomaddo:fig1`)?:(2.5cm)(`./no_maddo/computation_graph.pdf`){計算をグラフとして表す}

      +p {
        Pytorch・KerasなどPythonのDLフレームワークを使う・Neural Network Consoleなどのビジュアルエディタを用いる・AutoMLなどの自動モデル構築サービスを使うなど
        深層学習モデルを記述する方法は色々ありますが、本質的な部分では
        計算グラフというものは記述に用いるフレームワークに依存しないはずです。
      }

      +p {
        無論学習方法などはフレームワークごとに異なるのは普通だと思いますが、
        学習が終わったモデルとそのパラメータは学習に用いたフレームワークに非依存
        にしたいはずです\footnote{機械学習のステップとしては、一般的に事前に"学習"と呼ばれるパラメータを探索するステップがあり、その後そのパラメータを用いて"推論"と呼ばれるパラメータを固定して結果を得る計算を行います。}。Pythonで書かれたプログラムを学習に用いても、
        動かしたい環境は多岐に渡ります。
      }

      +p {
        例えば今想定される動作環境は以下のようなものがあります。
      }

      +enumerate {
        * NVIDIA GPUを搭載したサーバ環境
        * Intel HD Graphicsを搭載したノートブックのネイティブアプリケーション
        * Mali GPU / Adreno GPU などモバイルGPU搭載のAndroidデバイス
        * ブラウザ環境
        * 1チップ100円以下のマイコン（？）
        * FPGA上での実装（？）
      }

      +p {
        そこで計算内容を計算環境に非依存のフォーマットとランタイムに分離し、
        実行環境に合わせてランタイムが高速化するようなモデルが求められるわけです。
      }

      +p {
        ONNXの実行環境として有力なものに以下のものがあります。
        見事にMicrosoft開発なんですねこれが。
      }

      +enumerate {
        * ONNX Runtime \footnote {\url(`https://github.com/microsoft/onnxruntime`);}
        * ONNX.js \footnote {\url(`https://github.com/microsoft/onnxjs`);}
      }

      +p {
        すごく一般的な名前をしていますが特定企業の特定の実行環境の実装ですので
        公式のONNXと分けて考えましょう。
        これらはあとで使ってみます。特にONNX Runtime(以後onnxruntimeと書きます)はMicrosoftがちゃんとリソース割いて開発しているため
        多くの場合パフォーマンスもよく \footnote{時々デグレってますのでフレームワークの推論の実行結果とonnxruntime の実行結果必ず確認したほうがいいですよ。}  、確認の上使っても大丈夫なくらいにはなっていると思います。
      }

      +p {
        onnxruntimeはonnxの実行環境で、Python・C・Java・C\#・NodeJSなどのバインディングを持ちます。
        OpenMP CPU実行・NVIDIA GPU実行・OpenVINO
        \footnote{IntelのCPU/GPU/FPGA で高速実行するためのバックエンド。}
        実行などのバックエンドを持ち、自分で高速化のことを考えなくていいので楽ちんです。
      }
    >

    +section?:(`nomaddo:onnx_with_torch`){onnxruntimeとpytorch}<
      +p {
        最初にPytorchで書いたモデルからONNXフォーマットを出力する方法を確認しましょう。
        Pytorchは公式の実装としてonnxモジュールがあり、外部ツールを用いずとも
        onnxファイルを出力することができます。公式のチュートリアルもあるのですが
        上長な記述が多いので削ったものを示します。
      }

      +code?:(`nomaddo:torch_onnx`) (```
import torch
import torchvision

dummy_input = torch.randn(1, 3, 224, 224)
model = torchvision.models.alexnet(pretrained=True)

torch.onnx.export(model, dummy_input, "alexnet.onnx", verbose=True)
```){pretrained modelをonnx形式で保存する}

      +p {
        ポイントとしては、Pytorchのときはbatch sizeなど固定せずとも大丈夫だったパラメータを
        固定する必要があり、そのためにダミーの入力を渡すようになっています。
        \footnote{公式のチュートリアルではinput node, output nodeの名前を明示的に渡すようになっています。onnxruntimeをつかっていると必要がないのでここでは省いています}
        \footnote{\url(`https://pytorch.org/docs/stable/onnx.html#example-end-to-end-alexnet-from-pytorch-to-onnx`);}
      }

      +p {
        さて、ここでonnxファイルは手に入りました。実行するプログラムを見てみましょう。
        alexnetは入力画像の前処理が必要な上、PILから画像をロードするときはフォーマットの
        変換が必要なため思ったよりも長いプログラムになっています。
      }

      +code?:(`nomaddo:make_onnx_alexnet`)(```
import argparse
import onnx
import onnxruntime
import numpy as np
from PIL import Image

def main(args):
    # PILの画像フォーマットは HxWxCなのでCxHxWにtransposeする
    img = np.array(Image.open(args.img), dtype=np.float32)
    img = np.reshape(img, [1, 224, 224, 3]).transpose([0, 3, 1, 2]) / 255.0

    # torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    # 相当の正規化を行う
    # https://pytorch.org/hub/pytorch_vision_alexnet/
    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]
    for ch in range(3):
        img[0, ch, :, :] = (img[0, ch, :, :] - mean[ch]) / std[ch]

    # onnxruntimeを実行するコア部分、ココは覚えてね
    onnx.checker.check_model(onnx.load(args.model))
    sess = onnxruntime.InferenceSession(args.model)
    inputs = {sess.get_inputs()[0].name: img}
    outputs = [sess.get_outputs()[0].name]
    r = sess.run(outputs, inputs)
    print(r[0].reshape([1000]).argmax())

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='run alexnet')
    parser.add_argument('--model', type=str, default='./alexnet.onnx')
    parser.add_argument('img', type=str)
    args = parser.parse_args()
    main(args)
      ```){alexnet.onnxを実行する}

      +p {
        ちょっと長いのでonnxruntimeの使い方を確認しておきましょう。
      }

      +p {
        onnxファイルのパスからセッションを作成し、
        \code(`sess.run`); でモデルの推論を実行するために必要な計算グラフの
        入力ノード・出力ノードの名前を指定する必要があります。
        入力ノードには名前とそのノードに渡されるデータをdictにして一緒に渡します。
        これはグラフの入力ノードが複数ある場合・出力が複数ある場合にどれに渡す・
        どれをどの順番に受け取るのか混乱を避けるために必要なデータです。
        今回は1入力1出力のモデルですので、単純に\code(`sess`); に問い合わせて最初のものを
        利用すれば十分です。
      }

    >

    +section?:(`nomaddo:download_onnx`){onnxを扱うときの注意点}<
      +subsection?:(`nomaddo:pytorch_attention_point`){onnxが生成できるか}<
        +p {
          onnxファイルを生成するときに注意すべき事として、すべてのオペレータのエキスポートに対応している
          わけではないことがあります。例えばPytorchではmax_unpool2dはonnx出力に対応していません。
          このような情報はドキュメントには筆者は見つけられておらず、必ずonnxを用いたワークロードでは
          時間のかかる学習の前にエキスポートに対応しているか確認することが必要です。
        }

        +p {
          Pytorchはまだましで、公式にonnxを生成する機能のないフレームワークを用いる場合はonnxが作成しているonnx出力ツール・
          コミュニティの出力ツールを用いることになります。
          これらの品質は本当にてんでバラバラで何も考えずに信頼できるものではありません。
        }

        +p {
          例えば筆者はSonyが開発しているNeural Network Consoleにonnxエキスポート機能があるので試してみたことがあるのですが、
          生成されたonnxファイルは規格違反の要素が含まれておりonnxruntimeがエラーを吐きました。
          \footnote{\url(`https://groups.google.com/g/neural_network_console_users_jp/c/Jxb5jG1Ntnw/m/S9I1K_x5AwAJ?pli=1`); 直っているのか確認してはいません。}
          私が試した時、この問題はResnetをエキスポートする中で発生しており、DLソフトウエアスタックを開発している身からしてこのような有力なモデルアーキテクチャ
          でエラーが発生するということはonnxまわりの出力結果について何も確認をしていないのだなと受け取りました。
          機能としては存在するけども対して実行確認されていない、という状態のソフトウエアは何も信頼できないと思っておいたほうが
          業務では良いと思います。
        }

        +p {
          主なフレームワークとしてTensorflowがありますが、政治的な話として他のフレームワーク
          で使えるようにエキスポートすることを嫌うのか、公式でonnxに対応していません。
          onnxグループが作っているtensorflow-onnx
          \footnote{\url(`https://github.com/onnx/tensorflow-onnx`);}を使う事になります。
          Tensorflowを使うときにはTroubleShooting
          \footnote{\url(`https://github.com/onnx/tensorflow-onnx/blob/master/Troubleshooting.md`);}をよく読んでおいたほうが良いです。
          Tensorflow-Kerasも同様です。
        }
      >

      +subsection?:(`nomaddo:opset_version`){opset versionの存在}<
        +p {
          onnxにはopset versionという概念があり、onnx model zooにはonnxruntimeで実行できない
          ような古いバージョンのopset versionのonnxがあります。
          onnxruntimeのopset versionの対応状況は一覧にまとまっています\footnote{\url(`https://github.com/microsoft/onnxruntime/blob/master/docs/OperatorKernels.md`);}。
        }

        +p {
          onnxのレポジトリにはバージョンコンバーターが提供されるのですが、
          バージョンアップ自体に対応していないオペレータも存在しますので万能の存在ではありません
          \footnote{\url(`https://github.com/onnx/onnx/tree/master/onnx/version_converter`);}。
        }

        +p {
          今まではないと思いますが、onnxruntimeの状況によってはonnxの古いバージョンのサポートが
          切られる可能性もなくはないと思うので、自分で作成していないonnxファイルの取扱いにはリスクがあります。
          データは一度作成したら動かしたくないのに、バージョンアップでどんどん環境が変わっていくところがonnx、ひいては
          深層学習モデルとの付き合いで難しいところです。
          これはonnxだけではなく例えばtensorflowで書かれた古いモデルがだんだん実行できなくなっている、というのにも類似しています。
        }
      >
    >

    +section?:(`nomaddo:for_onnx`){onnx所感}<
      +p {
        onnx自体ツラミの塊みたいなフォーマットです。
        これは単にonnxを使うだけの人には関係がないことですが、面白いポイントだと思うのでまとめておきます。
      }

      +subsection?:(`nomaddo:onnx_intersection`){この世すべての悪の集合体}<
        +p {
          各DLフレームワークは互いのコンセンサスを取ることなくバラバラに開発されています。
          そのため各オペレータの演算の細かい挙動などことなるような実装が行われています。
          onnxはそれらの単なるいち"実装"を再現できるための"仕様"を含んでいます。
        }

        +p {
          例えばPaddingは画像に特定の値のピクセルを追加する処理ですが、
          tensorflowはバージョンごとに挙動が異なり\footnote{ほんまキレそう。}、Pytorch・Chainerは追加しようとしている軸の
          両側にピクセルを追加します。
          そのため多くのonnxのオペレータの\code(`auto_pad`); 属性にはそれらをすべて表現するために \code(`NOTSET`);, \code(`SAME_UPPER`); , \code(`SAME_LOWER`);, \code(`VALID`); の値を取るようにできています。
          DLフレームワークの実装が進めば進むほど負の遺産がonnxの仕様に集まってくるという特性があります。
        }

        +p {
          もちろんこれは単に使う人には関係がありませんが、
          onnxを利用したソフトウエアを開発する人には細々とした仕様全てに対応を強いられます。
        }
      >
      +subsection?:(`nomaddo:onnx_convension_violation`){onnxの仕様が守られない}<
        +p {
          onnxには一応仕様がありますが、守られない仕様があります。
          例えばonnxのグラフのノードの名前はC Identifierであることが
          定められていますが、多くの実装でPythonのオブジェクトのIDを
          そのまま名前に使いまわそうとしてこの名前の規格を起こしています。
          ONNX model zooですらこの規格違反のあるonnxを公開していました
          \footnote{\url(`https://github.com/onnx/models/issues/147`); }。
        }

        +p {
          Pytorchの実装はだいぶ前にグラフの名前がC identifierになっておらず、
          同僚が規格違反をIssueで連絡していて、\footnote{\url(`https://github.com/pytorch/pytorch/issues/30952`); }、
          そこで規格がなぜあるのか説いています。
        }

        +p {
          実装が先行する世界ではよくあることですが、
          規格だけではなく世の中の実装がどうなっているのか確認していく必要があります。
        }
      >
    >
    +section?:(`nomaddo:owarini`){あとがき}<
      +p {
        進捗大陸最終回ということで仕事で関わっているonnxについて書いてみました。
        たくさんのツラミがありつつも他にはないのでこれに頼らざるを得ないところがあります。
        やはり精力的に開発されている分野のツールはなんというか荒々しくて、
        ツールを使うのもなかなか大変です。
        どんどん進歩していく環境についていくだけで精一杯にならないように
        頑張りたいですね。
      }

      +p {
        今まではコンパイラを中心にソフトウエア開発をしていましたが、
        今後は書くソフトウエアの幅を広げていきたいと思っています。
        その一環として強化学習を試していたのですが、
        初めてやる強化学習の実装が全然うまくいかず
        この進捗大陸の現行は仕事でやった話をベースで書くことにしました。ちょっと残念です。
        やはり新しい分野でソフトウエアを書くのは大変で、もっと長い時間がかかる前提で
        取り組みべきでした。。。
      }
    >
  >
>